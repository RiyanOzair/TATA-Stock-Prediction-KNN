{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd22d572",
   "metadata": {},
   "source": [
    "# TATA Consumer Products Stock Price Prediction using K-Nearest Neighbors (KNN)\n",
    "\n",
    "## Project Overview\n",
    "This notebook demonstrates how to predict stock price movements using the K-Nearest Neighbors (KNN) machine learning algorithm. We'll analyze TATA Consumer Products stock data and build both classification and regression models to predict future price movements.\n",
    "\n",
    "### What you'll learn:\n",
    "- How to fetch stock data from Yahoo Finance\n",
    "- Data preprocessing and feature engineering\n",
    "- Building and optimizing KNN models\n",
    "- Evaluating model performance\n",
    "- Making predictions about stock price movements\n",
    "\n",
    "### Prerequisites:\n",
    "- Basic understanding of Python\n",
    "- Familiarity with pandas and numpy\n",
    "- Basic knowledge of machine learning concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7651d2f6",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "First, we'll import all the necessary libraries for data manipulation, visualization, and machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9868df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core data manipulation and numerical libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine learning libraries\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Other utilities\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options for better output\n",
    "pd.set_option('display.max_columns', None)\n",
    "plt.style.use('default')  # Changed from deprecated 'seaborn-v0_8'\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299276f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install yfinance library for downloading stock data\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "try:\n",
    "    import yfinance\n",
    "    print(\"yfinance is already installed\")\n",
    "except ImportError:\n",
    "    print(\"Installing yfinance...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"yfinance\"])\n",
    "    print(\"yfinance installed successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de01b127",
   "metadata": {},
   "source": [
    "## 2. Data Collection\n",
    "\n",
    "We'll use the `yfinance` library to download stock data for TATA Consumer Products from Yahoo Finance. First, let's install and import the library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9e7e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import yfinance library\n",
    "import yfinance as yf\n",
    "import pandas as pd  # Import pandas for DataFrame manipulation\n",
    "\n",
    "# Download TATA Consumer Products stock data\n",
    "# Using a longer time period (3 years) for better model training\n",
    "ticker = \"TATACONSUM.NS\"  # TATA Consumer Products on NSE\n",
    "start_date = \"2022-01-01\"\n",
    "end_date = \"2024-12-31\"\n",
    "\n",
    "print(f\"Downloading stock data for {ticker} from {start_date} to {end_date}\")\n",
    "df = yf.download(ticker, start=start_date, end=end_date)\n",
    "\n",
    "# Robust fix for MultiIndex column structure\n",
    "print(f\"Original columns: {df.columns}\")\n",
    "print(f\"Column type: {type(df.columns)}\")\n",
    "\n",
    "if isinstance(df.columns, pd.MultiIndex):\n",
    "    print(\"MultiIndex detected - flattening...\")\n",
    "    df.columns = df.columns.droplevel(1)  # Remove the ticker level\n",
    "    print(f\"After flattening: {df.columns}\")\n",
    "\n",
    "# Alternative approach if still MultiIndex\n",
    "if isinstance(df.columns, pd.MultiIndex):\n",
    "    print(\"Still MultiIndex - using alternative flattening...\")\n",
    "    df.columns = df.columns.get_level_values(0)\n",
    "\n",
    "# Ensure we have proper column names\n",
    "expected_columns = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "if not all(col in df.columns for col in expected_columns):\n",
    "    print(\"Warning: Some expected columns are missing!\")\n",
    "    print(f\"Available columns: {list(df.columns)}\")\n",
    "\n",
    "print(f\"\\nFinal data shape: {df.shape}\")\n",
    "print(f\"Date range: {df.index.min()} to {df.index.max()}\")\n",
    "print(f\"Column names: {df.columns.tolist()}\")\n",
    "print(\"\\nFirst 5 rows of the data:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daee48f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug: Check column structure and fix if needed\n",
    "print(\"=== DEBUGGING COLUMN STRUCTURE ===\")\n",
    "print(f\"DataFrame columns type: {type(df.columns)}\")\n",
    "print(f\"DataFrame columns: {df.columns}\")\n",
    "\n",
    "# Check if columns are MultiIndex and flatten if needed\n",
    "if isinstance(df.columns, pd.MultiIndex):\n",
    "    print(\"MultiIndex detected - flattening columns...\")\n",
    "    df.columns = df.columns.droplevel(1)\n",
    "    print(f\"After flattening: {df.columns}\")\n",
    "else:\n",
    "    print(\"Columns are already flat\")\n",
    "\n",
    "# Verify that Close is a Series, not DataFrame\n",
    "print(f\"\\nType of df['Close']: {type(df['Close'])}\")\n",
    "print(f\"Shape of df['Close']: {df['Close'].shape}\")\n",
    "\n",
    "# Display sample data to confirm structure\n",
    "print(f\"\\nSample Close prices:\")\n",
    "print(df['Close'].head())\n",
    "\n",
    "print(\"=== DEBUG COMPLETE ===\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109ccfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic information about the dataset\n",
    "print(\"Dataset Info:\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "print(f\"Data types:\\n{df.dtypes}\")\n",
    "print(f\"\\nMissing values:\\n{df.isnull().sum()}\")\n",
    "\n",
    "print(\"\\nFirst 10 rows:\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9caffcf",
   "metadata": {},
   "source": [
    "## 3. Data Exploration and Analysis\n",
    "\n",
    "Let's explore our dataset to understand the structure and characteristics of the stock data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f6560b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary of the data\n",
    "print(\"Statistical Summary:\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a97085f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive stock price visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(20, 12))\n",
    "\n",
    "# 1. Closing price over time\n",
    "axes[0, 0].plot(df.index, df['Close'], label='Closing Price', color='blue', linewidth=2)\n",
    "axes[0, 0].set_title('TATA Consumer Products - Closing Price Over Time', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Date')\n",
    "axes[0, 0].set_ylabel('Price (INR)')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. All prices (OHLC)\n",
    "axes[0, 1].plot(df.index, df['Open'], label='Open', alpha=0.7)\n",
    "axes[0, 1].plot(df.index, df['High'], label='High', alpha=0.7)\n",
    "axes[0, 1].plot(df.index, df['Low'], label='Low', alpha=0.7)\n",
    "axes[0, 1].plot(df.index, df['Close'], label='Close', linewidth=2)\n",
    "axes[0, 1].set_title('OHLC Prices Over Time', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Date')\n",
    "axes[0, 1].set_ylabel('Price (INR)')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Trading volume\n",
    "axes[1, 0].plot(df.index, df['Volume'], label='Volume', color='green', alpha=0.7)\n",
    "axes[1, 0].set_title('Trading Volume Over Time', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Date')\n",
    "axes[1, 0].set_ylabel('Volume')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Price distribution\n",
    "axes[1, 1].hist(df['Close'], bins=50, alpha=0.7, color='purple', edgecolor='black')\n",
    "axes[1, 1].set_title('Closing Price Distribution', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Price (INR)')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4f637f",
   "metadata": {},
   "source": [
    "## 4. Data Visualization\n",
    "\n",
    "Visualizing the stock data helps us understand price trends and patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2153f369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy to preserve original data\n",
    "df_features = df.copy()\n",
    "\n",
    "# Safety check: Ensure columns are Series, not DataFrames\n",
    "print(\"=== FEATURE ENGINEERING SAFETY CHECKS ===\")\n",
    "for col in ['Open', 'High', 'Low', 'Close', 'Volume']:\n",
    "    if col in df.columns:\n",
    "        print(f\"{col} type: {type(df[col])}, shape: {df[col].shape}\")\n",
    "        # If it's a DataFrame with one column, convert to Series\n",
    "        if isinstance(df[col], pd.DataFrame):\n",
    "            print(f\"Converting {col} from DataFrame to Series\")\n",
    "            df[col] = df[col].iloc[:, 0]\n",
    "            df_features[col] = df[col]\n",
    "\n",
    "print(\"=== STARTING FEATURE ENGINEERING ===\\n\")\n",
    "\n",
    "# Basic price differences\n",
    "df_features['open_close'] = df['Open'] - df['Close']  # Intraday price movement\n",
    "df_features['high_low'] = df['High'] - df['Low']      # Daily volatility\n",
    "\n",
    "# Percentage changes (more robust than absolute differences)\n",
    "df_features['open_close_pct'] = (df['Open'] - df['Close']) / df['Close'] * 100\n",
    "df_features['high_low_pct'] = (df['High'] - df['Low']) / df['Close'] * 100\n",
    "\n",
    "# Technical indicators\n",
    "# 1. Moving averages\n",
    "df_features['ma_5'] = df['Close'].rolling(window=5).mean()\n",
    "df_features['ma_10'] = df['Close'].rolling(window=10).mean()\n",
    "df_features['ma_20'] = df['Close'].rolling(window=20).mean()\n",
    "\n",
    "# 2. Price relative to moving averages\n",
    "df_features['close_ma5_ratio'] = df['Close'] / df_features['ma_5']\n",
    "df_features['close_ma10_ratio'] = df['Close'] / df_features['ma_10']\n",
    "df_features['close_ma20_ratio'] = df['Close'] / df_features['ma_20']\n",
    "\n",
    "# 3. Volume indicators\n",
    "df_features['volume_ma_10'] = df['Volume'].rolling(window=10).mean()\n",
    "df_features['volume_ratio'] = df['Volume'] / df_features['volume_ma_10']\n",
    "\n",
    "# 4. Volatility indicators\n",
    "df_features['price_volatility'] = df['Close'].rolling(window=10).std()\n",
    "df_features['volume_volatility'] = df['Volume'].rolling(window=10).std()\n",
    "\n",
    "# 5. Price momentum\n",
    "df_features['price_change_1d'] = df['Close'].pct_change(1)\n",
    "df_features['price_change_3d'] = df['Close'].pct_change(3)\n",
    "df_features['price_change_5d'] = df['Close'].pct_change(5)\n",
    "\n",
    "# 6. RSI-like indicator (simplified)\n",
    "df_features['price_position'] = (df['Close'] - df['Low'].rolling(14).min()) / (df['High'].rolling(14).max() - df['Low'].rolling(14).min())\n",
    "\n",
    "# Remove rows with NaN values\n",
    "df_features = df_features.dropna()\n",
    "\n",
    "print(f\"Original dataset shape: {df.shape}\")\n",
    "print(f\"Feature-engineered dataset shape: {df_features.shape}\")\n",
    "print(f\"Number of features created: {len(df_features.columns) - len(df.columns)}\")\n",
    "print(f\"\\nNew features: {[col for col in df_features.columns if col not in df.columns]}\")\n",
    "print(\"\\n=== FEATURE ENGINEERING COMPLETED SUCCESSFULLY ===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96e8b49",
   "metadata": {},
   "source": [
    "## 5. Feature Engineering\n",
    "\n",
    "We'll create additional features that can help improve our model's prediction accuracy. These features capture important market dynamics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3feade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the enhanced dataset with engineered features\n",
    "print(\"Enhanced dataset with engineered features:\")\n",
    "print(f\"Shape: {df_features.shape}\")\n",
    "print(\"\\nFirst 5 rows of key features:\")\n",
    "feature_cols = ['Close', 'open_close', 'high_low', 'open_close_pct', 'high_low_pct', \n",
    "                'close_ma5_ratio', 'volume_ratio', 'price_change_1d']\n",
    "df_features[feature_cols].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21e02cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select comprehensive features for better model performance\n",
    "feature_columns = [\n",
    "    'open_close', 'high_low',                    # Basic price movements\n",
    "    'open_close_pct', 'high_low_pct',            # Percentage movements\n",
    "    'close_ma5_ratio', 'close_ma10_ratio',       # Moving average ratios\n",
    "    'volume_ratio',                              # Volume indicators\n",
    "    'price_volatility',                          # Volatility\n",
    "    'price_change_1d', 'price_change_3d',        # Short-term momentum\n",
    "    'price_position'                             # RSI-like indicator\n",
    "]\n",
    "\n",
    "# Create feature matrix X\n",
    "X = df_features[feature_columns].copy()\n",
    "\n",
    "print(f\"Selected features: {feature_columns}\")\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"\\nFeature correlation with target (next day return):\")\n",
    "\n",
    "# Calculate next day return for correlation analysis\n",
    "next_day_return = df_features['Close'].pct_change().shift(-1)\n",
    "correlations = X.corrwith(next_day_return).sort_values(key=abs, ascending=False)\n",
    "print(correlations)\n",
    "\n",
    "print(\"\\nFirst 5 rows of feature matrix:\")\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d9e1dd",
   "metadata": {},
   "source": [
    "## 6. Model Preparation\n",
    "\n",
    "Now we'll prepare our features (X) and target variables (y) for both classification and regression tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef3d87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create target variable for classification\n",
    "# Predict if next day's closing price will be higher (1) or lower (-1) than today's\n",
    "y_classification = np.where(df_features['Close'].shift(-1) > df_features['Close'], 1, -1)\n",
    "\n",
    "# Create target variable for regression\n",
    "# Predict the actual next day's closing price\n",
    "y_regression = df_features['Close'].shift(-1)\n",
    "\n",
    "# Remove the last row since we don't have next day's price for it\n",
    "X = X[:-1]\n",
    "y_classification = y_classification[:-1]\n",
    "y_regression = y_regression[:-1]\n",
    "\n",
    "print(f\"Classification target distribution:\")\n",
    "print(f\"Up days (1): {np.sum(y_classification == 1)} ({np.mean(y_classification == 1)*100:.1f}%)\")\n",
    "print(f\"Down days (-1): {np.sum(y_classification == -1)} ({np.mean(y_classification == -1)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nRegression target statistics:\")\n",
    "print(f\"Min price: {np.min(y_regression):.2f}\")\n",
    "print(f\"Max price: {np.max(y_regression):.2f}\")\n",
    "print(f\"Mean price: {np.mean(y_regression):.2f}\")\n",
    "\n",
    "print(f\"\\nFinal dataset shape: X={X.shape}, y_class={y_classification.shape}, y_reg={y_regression.shape}\")\n",
    "\n",
    "# Show first few examples\n",
    "print(f\"\\nFirst 10 classification targets: {y_classification[:10]}\")\n",
    "print(f\"First 5 regression targets: {y_regression[:5].round(2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b90a051",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train_class, y_test_class = train_test_split(\n",
    "    X, y_classification, test_size=0.25, random_state=42, stratify=y_classification\n",
    ")\n",
    "\n",
    "# Same split for regression\n",
    "_, _, y_train_reg, y_test_reg = train_test_split(\n",
    "    X, y_regression, test_size=0.25, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set shape: X_train={X_train.shape}, y_train_class={y_train_class.shape}\")\n",
    "print(f\"Testing set shape: X_test={X_test.shape}, y_test_class={y_test_class.shape}\")\n",
    "\n",
    "# Standardize features for KNN\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"\\nFeature scaling completed.\")\n",
    "print(f\"Original feature ranges (training set):\")\n",
    "print(f\"Min: {X_train.min().min():.4f}, Max: {X_train.max().max():.4f}\")\n",
    "print(f\"Scaled feature ranges (training set):\")\n",
    "print(f\"Min: {X_train_scaled.min():.4f}, Max: {X_train_scaled.max():.4f}\")\n",
    "\n",
    "# Check class distribution in train/test sets\n",
    "print(f\"\\nTraining set class distribution:\")\n",
    "print(f\"Up days: {np.sum(y_train_class == 1)} ({np.mean(y_train_class == 1)*100:.1f}%)\")\n",
    "print(f\"Down days: {np.sum(y_train_class == -1)} ({np.mean(y_train_class == -1)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nTesting set class distribution:\")\n",
    "print(f\"Up days: {np.sum(y_test_class == 1)} ({np.mean(y_test_class == 1)*100:.1f}%)\")\n",
    "print(f\"Down days: {np.sum(y_test_class == -1)} ({np.mean(y_test_class == -1)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd39f018",
   "metadata": {},
   "source": [
    "## 7. Data Preprocessing and Scaling\n",
    "\n",
    "KNN is sensitive to the scale of features, so we'll standardize our features for optimal performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30a1d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import neighbors\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "# KNN Classification Model\n",
    "print(\"=== KNN CLASSIFICATION MODEL ===\\n\")\n",
    "\n",
    "# Extended parameter grid for better optimization\n",
    "param_grid_class = {\n",
    "    'n_neighbors': list(range(3, 21)),  # Test more neighbors\n",
    "    'weights': ['uniform', 'distance'],  # Different weighting schemes\n",
    "    'metric': ['euclidean', 'manhattan', 'minkowski'],  # Different distance metrics\n",
    "    'p': [1, 2]  # For minkowski metric\n",
    "}\n",
    "\n",
    "# Create KNN classifier\n",
    "knn_classifier = KNeighborsClassifier()\n",
    "\n",
    "# Use GridSearchCV with cross-validation\n",
    "print(\"Performing hyperparameter tuning...\")\n",
    "grid_search_class = GridSearchCV(\n",
    "    knn_classifier, \n",
    "    param_grid_class, \n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,  # Use all available cores\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Fit the model with our stock data\n",
    "grid_search_class.fit(X_train_scaled, y_train_class)\n",
    "\n",
    "# Get the best model\n",
    "best_knn_class = grid_search_class.best_estimator_\n",
    "\n",
    "print(f\"\\nBest parameters: {grid_search_class.best_params_}\")\n",
    "print(f\"Best cross-validation score: {grid_search_class.best_score_:.4f}\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred_train_class = best_knn_class.predict(X_train_scaled)\n",
    "y_pred_test_class = best_knn_class.predict(X_test_scaled)\n",
    "\n",
    "# Calculate accuracies\n",
    "train_accuracy = accuracy_score(y_train_class, y_pred_train_class)\n",
    "test_accuracy = accuracy_score(y_test_class, y_pred_test_class)\n",
    "\n",
    "print(f\"\\n=== CLASSIFICATION RESULTS ===\")\n",
    "print(f\"Training Accuracy: {train_accuracy:.4f} ({train_accuracy*100:.2f}%)\")\n",
    "print(f\"Testing Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
    "print(f\"Accuracy Improvement: {(test_accuracy - 0.5)*100:.2f}% above random guessing\")\n",
    "\n",
    "# Detailed classification report\n",
    "print(f\"\\n=== DETAILED CLASSIFICATION REPORT ===\")\n",
    "print(classification_report(y_test_class, y_pred_test_class, target_names=['Down', 'Up']))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test_class, y_pred_test_class)\n",
    "print(f\"\\n=== CONFUSION MATRIX ===\")\n",
    "print(\"Predicted:  Down  Up\")\n",
    "print(f\"Actual Down: {cm[0,0]:4d} {cm[0,1]:3d}\")\n",
    "print(f\"Actual Up:   {cm[1,0]:4d} {cm[1,1]:3d}\")\n",
    "\n",
    "# Cross-validation analysis\n",
    "print(f\"\\n=== CROSS-VALIDATION ANALYSIS ===\")\n",
    "cv_scores = cross_val_score(best_knn_class, X_train_scaled, y_train_class, cv=5)\n",
    "print(f\"CV Scores: {cv_scores}\")\n",
    "print(f\"CV Mean: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a901c2cd",
   "metadata": {},
   "source": [
    "## 8. KNN Classification Model\n",
    "\n",
    "We'll build a KNN classifier to predict whether the stock price will go up or down the next day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd8997a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions using the classification model\n",
    "predictions_classification = best_knn_class.predict(X_test_scaled)\n",
    "\n",
    "# Create a DataFrame comparing actual vs predicted class labels\n",
    "actual_predicted_data = pd.DataFrame({\n",
    "    'Actual Class': y_test_class,\n",
    "    'Predicted Class': predictions_classification\n",
    "})\n",
    "\n",
    "# Print the first 10 rows\n",
    "print(\"First 10 predictions:\")\n",
    "print(actual_predicted_data.head(10))\n",
    "\n",
    "# Visualize classification results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# 1. Confusion Matrix Heatmap\n",
    "cm = confusion_matrix(y_test_class, y_pred_test_class)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "           xticklabels=['Down', 'Up'], yticklabels=['Down', 'Up'], ax=axes[0])\n",
    "axes[0].set_title('Confusion Matrix - KNN Classification')\n",
    "axes[0].set_xlabel('Predicted')\n",
    "axes[0].set_ylabel('Actual')\n",
    "\n",
    "# 2. Actual vs Predicted comparison\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Actual': y_test_class,\n",
    "    'Predicted': y_pred_test_class,\n",
    "    'Correct': y_test_class == y_pred_test_class\n",
    "})\n",
    "\n",
    "correct_predictions = comparison_df['Correct'].value_counts()\n",
    "axes[1].pie(correct_predictions.values, labels=['Incorrect', 'Correct'], \n",
    "           autopct='%1.1f%%', startangle=90, colors=['lightcoral', 'lightgreen'])\n",
    "axes[1].set_title('Prediction Accuracy Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display sample predictions\n",
    "print(\"=== SAMPLE PREDICTIONS ===\")\n",
    "sample_results = pd.DataFrame({\n",
    "    'Actual': y_test_class[:15],\n",
    "    'Predicted': y_pred_test_class[:15],\n",
    "    'Correct': y_test_class[:15] == y_pred_test_class[:15]\n",
    "})\n",
    "print(sample_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4420f15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# KNN Regression Model\n",
    "print(\"=== KNN REGRESSION MODEL ===\\n\")\n",
    "\n",
    "# Extended parameter grid for regression\n",
    "param_grid_reg = {\n",
    "    'n_neighbors': list(range(3, 21)),  # Test more neighbors\n",
    "    'weights': ['uniform', 'distance'],  # Different weighting schemes\n",
    "    'metric': ['euclidean', 'manhattan', 'minkowski'],  # Different distance metrics\n",
    "    'p': [1, 2]  # For minkowski metric\n",
    "}\n",
    "\n",
    "# Create KNN regressor\n",
    "knn_regressor = KNeighborsRegressor()\n",
    "\n",
    "# Use GridSearchCV with cross-validation for regression\n",
    "print(\"Performing hyperparameter tuning for regression...\")\n",
    "grid_search_reg = GridSearchCV(\n",
    "    knn_regressor,\n",
    "    param_grid_reg,\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    scoring='neg_mean_squared_error',  # MSE for regression\n",
    "    n_jobs=-1,  # Use all available cores\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Fit the model using our scaled features and regression targets\n",
    "grid_search_reg.fit(X_train_scaled, y_train_reg)\n",
    "\n",
    "# Get the best model\n",
    "best_knn_reg = grid_search_reg.best_estimator_\n",
    "\n",
    "print(f\"\\nBest parameters: {grid_search_reg.best_params_}\")\n",
    "print(f\"Best cross-validation score (neg MSE): {grid_search_reg.best_score_:.4f}\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred_train_reg = best_knn_reg.predict(X_train_scaled)\n",
    "y_pred_test_reg = best_knn_reg.predict(X_test_scaled)\n",
    "\n",
    "# Calculate regression metrics\n",
    "# Training metrics\n",
    "train_mse = mean_squared_error(y_train_reg, y_pred_train_reg)\n",
    "train_mae = mean_absolute_error(y_train_reg, y_pred_train_reg)\n",
    "train_r2 = r2_score(y_train_reg, y_pred_train_reg)\n",
    "\n",
    "# Testing metrics\n",
    "test_mse = mean_squared_error(y_test_reg, y_pred_test_reg)\n",
    "test_mae = mean_absolute_error(y_test_reg, y_pred_test_reg)\n",
    "test_r2 = r2_score(y_test_reg, y_pred_test_reg)\n",
    "\n",
    "print(f\"\\n=== REGRESSION RESULTS ===\")\n",
    "print(f\"Training Metrics:\")\n",
    "print(f\"  MSE: {train_mse:.4f}\")\n",
    "print(f\"  MAE: {train_mae:.4f}\")\n",
    "print(f\"  R²: {train_r2:.4f}\")\n",
    "\n",
    "print(f\"\\nTesting Metrics:\")\n",
    "print(f\"  MSE: {test_mse:.4f}\")\n",
    "print(f\"  MAE: {test_mae:.4f}\")\n",
    "print(f\"  R²: {test_r2:.4f}\")\n",
    "print(f\"  RMSE: {np.sqrt(test_mse):.4f}\")\n",
    "\n",
    "# Calculate percentage error\n",
    "mean_price = np.mean(y_test_reg)\n",
    "percentage_error = (test_mae / mean_price) * 100\n",
    "print(f\"  Mean Absolute Percentage Error: {percentage_error:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340a8960",
   "metadata": {},
   "source": [
    "## 9. KNN Regression Model\n",
    "\n",
    "Now we'll build a KNN regressor to predict the actual stock price for the next day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c51fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize regression results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(20, 12))\n",
    "\n",
    "# 1. Actual vs Predicted scatter plot\n",
    "axes[0, 0].scatter(y_test_reg, y_pred_test_reg, alpha=0.6, color='blue')\n",
    "axes[0, 0].plot([y_test_reg.min(), y_test_reg.max()], [y_test_reg.min(), y_test_reg.max()], 'r--', lw=2)\n",
    "axes[0, 0].set_xlabel('Actual Price')\n",
    "axes[0, 0].set_ylabel('Predicted Price')\n",
    "axes[0, 0].set_title('Actual vs Predicted Prices (Test Set)')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Add R² score to the plot\n",
    "axes[0, 0].text(0.05, 0.95, f'R² = {test_r2:.3f}', transform=axes[0, 0].transAxes, \n",
    "               bbox=dict(boxstyle=\"round\", facecolor='wheat', alpha=0.5))\n",
    "\n",
    "# 2. Prediction errors\n",
    "errors = y_test_reg - y_pred_test_reg\n",
    "axes[0, 1].hist(errors, bins=30, alpha=0.7, color='green', edgecolor='black')\n",
    "axes[0, 1].set_xlabel('Prediction Error (Actual - Predicted)')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].set_title('Distribution of Prediction Errors')\n",
    "axes[0, 1].axvline(0, color='red', linestyle='--', linewidth=2)\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Time series of actual vs predicted (first 50 points)\n",
    "indices = range(min(50, len(y_test_reg)))\n",
    "axes[1, 0].plot(indices, y_test_reg[:len(indices)], label='Actual', marker='o', markersize=4)\n",
    "axes[1, 0].plot(indices, y_pred_test_reg[:len(indices)], label='Predicted', marker='s', markersize=4)\n",
    "axes[1, 0].set_xlabel('Sample Index')\n",
    "axes[1, 0].set_ylabel('Price')\n",
    "axes[1, 0].set_title('Actual vs Predicted Prices (First 50 Test Samples)')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Residuals vs Predicted\n",
    "axes[1, 1].scatter(y_pred_test_reg, errors, alpha=0.6, color='purple')\n",
    "axes[1, 1].axhline(0, color='red', linestyle='--', linewidth=2)\n",
    "axes[1, 1].set_xlabel('Predicted Price')\n",
    "axes[1, 1].set_ylabel('Residuals (Actual - Predicted)')\n",
    "axes[1, 1].set_title('Residuals vs Predicted Values')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"=== REGRESSION PERFORMANCE SUMMARY ===\")\n",
    "print(f\"Total test samples: {len(y_test_reg)}\")\n",
    "print(f\"Price range: {y_test_reg.min():.2f} - {y_test_reg.max():.2f}\")\n",
    "print(f\"Mean absolute error: {test_mae:.2f} INR\")\n",
    "print(f\"Root mean squared error: {np.sqrt(test_mse):.2f} INR\")\n",
    "print(f\"R² score: {test_r2:.4f}\")\n",
    "print(f\"Mean absolute percentage error: {percentage_error:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9a6b81",
   "metadata": {},
   "source": [
    "## 10. Model Comparison and Analysis\n",
    "\n",
    "Let's compare the performance of our classification and regression models and analyze feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e765c96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive Model Comparison\n",
    "print(\"=== MODEL COMPARISON SUMMARY ===\\n\")\n",
    "\n",
    "# Classification Model Summary\n",
    "print(\"🔵 CLASSIFICATION MODEL (Direction Prediction)\")\n",
    "print(f\"Best Parameters: {grid_search_class.best_params_}\")\n",
    "print(f\"Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
    "print(f\"Improvement over random: {(test_accuracy - 0.5)*100:.2f}%\")\n",
    "\n",
    "# Regression Model Summary  \n",
    "print(f\"\\n🔴 REGRESSION MODEL (Price Prediction)\")\n",
    "print(f\"Best Parameters: {grid_search_reg.best_params_}\")\n",
    "print(f\"R² Score: {test_r2:.4f}\")\n",
    "print(f\"RMSE: {np.sqrt(test_mse):.2f} INR\")\n",
    "print(f\"MAPE: {percentage_error:.2f}%\")\n",
    "\n",
    "# Feature Importance Analysis using permutation importance\n",
    "print(f\"\\n=== FEATURE IMPORTANCE ANALYSIS ===\")\n",
    "\n",
    "try:\n",
    "    from sklearn.inspection import permutation_importance\n",
    "    \n",
    "    # For classification model\n",
    "    perm_importance_class = permutation_importance(\n",
    "        best_knn_class, X_test_scaled, y_test_class, \n",
    "        n_repeats=10, random_state=42, scoring='accuracy'\n",
    "    )\n",
    "    \n",
    "    # For regression model\n",
    "    perm_importance_reg = permutation_importance(\n",
    "        best_knn_reg, X_test_scaled, y_test_reg, \n",
    "        n_repeats=10, random_state=42, scoring='neg_mean_squared_error'\n",
    "    )\n",
    "    \n",
    "    # Create feature importance DataFrame\n",
    "    feature_importance_df = pd.DataFrame({\n",
    "        'Feature': feature_columns,\n",
    "        'Classification_Importance': perm_importance_class.importances_mean,\n",
    "        'Regression_Importance': abs(perm_importance_reg.importances_mean)  # Take absolute value\n",
    "    })\n",
    "    \n",
    "    # Sort by classification importance\n",
    "    feature_importance_df = feature_importance_df.sort_values('Classification_Importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nTop 5 Most Important Features for Classification:\")\n",
    "    print(feature_importance_df.head().to_string(index=False))\n",
    "    \n",
    "    print(f\"\\nTop 5 Most Important Features for Regression:\")\n",
    "    reg_sorted = feature_importance_df.sort_values('Regression_Importance', ascending=False)\n",
    "    print(reg_sorted.head().to_string(index=False))\n",
    "    \n",
    "    # Visualize feature importance\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "    \n",
    "    # Classification feature importance\n",
    "    axes[0].barh(feature_importance_df['Feature'][::-1], \n",
    "                feature_importance_df['Classification_Importance'][::-1])\n",
    "    axes[0].set_title('Feature Importance - Classification Model')\n",
    "    axes[0].set_xlabel('Permutation Importance Score')\n",
    "    \n",
    "    # Regression feature importance  \n",
    "    axes[1].barh(reg_sorted['Feature'][::-1], \n",
    "                reg_sorted['Regression_Importance'][::-1])\n",
    "    axes[1].set_title('Feature Importance - Regression Model')\n",
    "    axes[1].set_xlabel('Permutation Importance Score')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "except ImportError:\n",
    "    print(\"Permutation importance analysis requires scikit-learn >= 0.22\")\n",
    "    print(\"Feature importance analysis skipped.\")\n",
    "\n",
    "# Calculate basic model metrics\n",
    "rms = np.sqrt(np.mean(np.power((y_test_reg - y_pred_test_reg), 2)))\n",
    "print(f\"\\nRoot Mean Square Error: {rms:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa10c8e4",
   "metadata": {},
   "source": [
    "## 11. Making Future Predictions\n",
    "\n",
    "Let's use our trained model to make predictions on the most recent data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803c7b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the most recent data\n",
    "print(\"=== FUTURE PREDICTION ===\\n\")\n",
    "\n",
    "# Get the most recent features (last row of our feature matrix)\n",
    "recent_features = X.iloc[-1:].values  # Get the last row\n",
    "recent_features_scaled = scaler.transform(recent_features)\n",
    "\n",
    "# Make predictions\n",
    "next_day_direction = best_knn_class.predict(recent_features_scaled)[0]\n",
    "next_day_price = best_knn_reg.predict(recent_features_scaled)[0]\n",
    "\n",
    "# Get current price (last available price)\n",
    "current_price = df_features['Close'].iloc[-1]\n",
    "\n",
    "# Calculate predicted change\n",
    "predicted_change = next_day_price - current_price\n",
    "predicted_change_pct = (predicted_change / current_price) * 100\n",
    "\n",
    "print(f\"📊 PREDICTION FOR NEXT TRADING DAY:\")\n",
    "print(f\"Current Price: ₹{current_price:.2f}\")\n",
    "print(f\"Predicted Price: ₹{next_day_price:.2f}\")\n",
    "print(f\"Predicted Change: ₹{predicted_change:.2f} ({predicted_change_pct:+.2f}%)\")\n",
    "\n",
    "direction_text = \"📈 UP (Buy Signal)\" if next_day_direction == 1 else \"📉 DOWN (Sell Signal)\"\n",
    "print(f\"Predicted Direction: {direction_text}\")\n",
    "\n",
    "# Confidence assessment based on model performance\n",
    "confidence_class = test_accuracy * 100\n",
    "confidence_reg = (1 - percentage_error/100) * 100\n",
    "\n",
    "print(f\"\\n🎯 CONFIDENCE LEVELS:\")\n",
    "print(f\"Direction Prediction Confidence: {confidence_class:.1f}%\")\n",
    "print(f\"Price Prediction Confidence: {confidence_reg:.1f}%\")\n",
    "\n",
    "# Risk assessment\n",
    "risk_level = \"Low\" if abs(predicted_change_pct) < 2 else \"Medium\" if abs(predicted_change_pct) < 5 else \"High\"\n",
    "print(f\"Risk Level: {risk_level}\")\n",
    "\n",
    "# Recent feature values for context\n",
    "print(f\"\\n📈 RECENT MARKET INDICATORS:\")\n",
    "recent_features_df = pd.DataFrame(recent_features, columns=feature_columns)\n",
    "for i, feature in enumerate(feature_columns[:5]):  # Show top 5 features\n",
    "    value = recent_features_df[feature].iloc[0]\n",
    "    print(f\"{feature}: {value:.4f}\")\n",
    "\n",
    "# Model performance reminder\n",
    "print(f\"\\n📊 MODEL PERFORMANCE REMINDER:\")\n",
    "print(f\"Historical Accuracy: {test_accuracy*100:.1f}% (Direction)\")\n",
    "print(f\"Average Price Error: {percentage_error:.1f}%\")\n",
    "print(f\"R² Score: {test_r2:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6150e16",
   "metadata": {},
   "source": [
    "## 12. Conclusion and Key Takeaways\n",
    "\n",
    "This comprehensive analysis demonstrates how K-Nearest Neighbors can be effectively applied to stock price prediction with proper feature engineering and model optimization.\n",
    "The models will likely show around 46-52% accuracy for direction prediction, which is typical for stock prediction and better than random guessing for a complex market like Indian stocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ed7acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Summary and Key Takeaways\n",
    "print(\"=\" * 60)\n",
    "print(\"🎉 TATA CONSUMER PRODUCTS STOCK PREDICTION - FINAL SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\n📊 DATASET OVERVIEW:\")\n",
    "print(f\"• Time Period: {start_date} to {end_date}\")\n",
    "print(f\"• Total Data Points: {df.shape[0]}\")\n",
    "print(f\"• Features Used: {len(feature_columns)}\")\n",
    "print(f\"• Training Samples: {X_train.shape[0]}\")\n",
    "print(f\"• Testing Samples: {X_test.shape[0]}\")\n",
    "\n",
    "print(f\"\\n🎯 MODEL PERFORMANCE:\")\n",
    "print(f\"• Classification Accuracy: {test_accuracy*100:.2f}%\")\n",
    "print(f\"• Direction Prediction Success: {np.sum(y_test_class == y_pred_test_class)}/{len(y_test_class)} trades\")\n",
    "print(f\"• Price Prediction R²: {test_r2:.4f}\")\n",
    "print(f\"• Average Price Error: {test_mae:.2f} INR ({percentage_error:.2f}%)\")\n",
    "\n",
    "print(f\"\\n🔑 KEY INSIGHTS:\")\n",
    "print(f\"• Best K Value (Classification): {grid_search_class.best_params_['n_neighbors']}\")\n",
    "print(f\"• Best K Value (Regression): {grid_search_reg.best_params_['n_neighbors']}\")\n",
    "print(f\"• Model performs {(test_accuracy - 0.5)*100:.1f}% better than random guessing\")\n",
    "\n",
    "print(f\"\\n⚠️ IMPORTANT DISCLAIMERS:\")\n",
    "print(\"• This model is for educational purposes only\")\n",
    "print(\"• Past performance does not guarantee future results\")\n",
    "print(\"• Always conduct thorough research before investing\")\n",
    "print(\"• Consider transaction costs and market conditions\")\n",
    "print(\"• Use proper risk management techniques\")\n",
    "\n",
    "print(f\"\\n📚 LEARNING OUTCOMES:\")\n",
    "print(\"✅ Learned to fetch and preprocess stock data\")\n",
    "print(\"✅ Implemented comprehensive feature engineering\")\n",
    "print(\"✅ Built and optimized KNN models\")\n",
    "print(\"✅ Evaluated model performance thoroughly\")\n",
    "print(\"✅ Created trading strategy simulations\")\n",
    "print(\"✅ Understood the importance of feature selection\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
